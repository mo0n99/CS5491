{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ba1915fced4e72",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Run FunSearch on Capacitated Vehicle Routing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e93216-c6da-4283-bd87-19d2c927ea01",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "Our project is to apply [FunSearch](https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/) on [Capacitated Vehicle Routing Problem](https://developers.google.com/optimization/routing/cvrp)(CVRP) to find better heuristic algorithm.\n",
    "\n",
    "Instead of relying on human intuitions or manual trial-and-errors to find a better heuristic, **FunSearch**  automate the design process by treating it as a search problem.\n",
    "\n",
    "FunSearch refers to a heuristic function as a program. FunSearch first asks an LLM to create a set \n",
    "of programs, each of which will be incorporated into the general heuristic code templatve. Each program will be evaluated on some test instancms \r\n",
    "and the resulting performance will be recorded. Then, the main loop of FunSearch starts, where n \r\n",
    "each iteration, FunSearch picks a promising pronce) from the setof \r\n",
    "programs created before. This program will be sent to an LLM for further modifications so thatthe \r\n",
    "performance can be improved. Then, the modified program is evaroblem), and it will replace the original program f its \r\n",
    "performance improves. This process will repeat for many iterations until a novel program is otained \r\n",
    "or some pre-defined max # of iterations is\n",
    "\n",
    "This project is divided into 5 steps:\n",
    "1. Implement 'LLM' interface.\n",
    "2. Implement a 'SandBox' interface.\n",
    "3. Prepare a 'specification'.\n",
    "4. Prepare a dataset.\n",
    "5. Start FunSearch.\n",
    "\n",
    "**Caution**\n",
    "\n",
    "This document references https://github.com/RayZhhh/funsearchFunSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d02b8e9c3ba67",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Preparation: download the project file from github. And update system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22453e8153e0934c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/RayZhhh/funsearch.git\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/content/funsearch/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47175708cc0a93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. Implement LLM interface\n",
    "Set the API's IP address according to your API provider (See line 65 in the following code).\n",
    "```python\n",
    "conn = http.client.HTTPSConnection(\"api.chatanywhere.com.cn\")\n",
    "```\n",
    "You should prepare a 'key' for the LLM API. And fill them in the header (See line 76-80 in the following code).\n",
    "```python\n",
    "headers = {\n",
    "    'Authorization': 'Bearer [put your key here, the key may start with \"sk-...\"]',\n",
    "    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "```\n",
    "**Caution**\n",
    "The LLM interface implementation references https://github.com/RayZhhh/funsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999e45c9a568b08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import multiprocessing\n",
    "from typing import Collection, Any\n",
    "import http.client\n",
    "from implementation import sampler\n",
    "\n",
    "\n",
    "def _trim_preface_of_body(sample: str) -> str:\n",
    "    \"\"\"Trim the redundant descriptions/symbols/'def' declaration before the function body.\n",
    "    Please see my comments in sampler.LLM (in sampler.py).\n",
    "    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.\n",
    "\n",
    "    -Example sample (function & description generated by LLM):\n",
    "    -------------------------------------\n",
    "    This is the optimized function ...\n",
    "    def priority_v2(...) -> ...:\n",
    "        return ...\n",
    "    This function aims to ...\n",
    "    -------------------------------------\n",
    "    -This function removes the description above the function's signature, and the function's signature.\n",
    "    -The indent of the code is preserved.\n",
    "    -Return of this function:\n",
    "    -------------------------------------\n",
    "        return ...\n",
    "    This function aims to ...\n",
    "    -------------------------------------\n",
    "    \"\"\"\n",
    "    lines = sample.splitlines()\n",
    "    func_body_lineno = 0\n",
    "    find_def_declaration = False\n",
    "    for lineno, line in enumerate(lines):\n",
    "        # find the first 'def' statement in the given code\n",
    "        if line[:3] == 'def':\n",
    "            func_body_lineno = lineno\n",
    "            find_def_declaration = True\n",
    "            break\n",
    "    if find_def_declaration:\n",
    "        code = ''\n",
    "        for line in lines[func_body_lineno + 1:]:\n",
    "            code += line + '\\n'\n",
    "        return code\n",
    "    return sample\n",
    "\n",
    "\n",
    "class LLMAPI(sampler.LLM):\n",
    "    \"\"\"Language model that predicts continuation of provided source code.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, samples_per_prompt: int, trim=True):\n",
    "        super().__init__(samples_per_prompt)\n",
    "        additional_prompt = ('You are an expert in optimization of CVRP, generate a specific improvement strategy based on the current state of the solution.'\n",
    "                             'Complete a different and more complex Python function. '\n",
    "                             'Be creative and you can insert multiple if-else and for-loop in the code logic.'\n",
    "                             'Only output the Python code, no descriptions.')\n",
    "        self._additional_prompt = additional_prompt\n",
    "        self._trim = trim\n",
    "\n",
    "    def draw_samples(self, prompt: str) -> Collection[str]:\n",
    "        \"\"\"Returns multiple predicted continuations of `prompt`.\"\"\"\n",
    "        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]\n",
    "\n",
    "    def _draw_sample(self, content: str) -> str:\n",
    "        prompt = '\\n'.join([content, self._additional_prompt])\n",
    "        while True:\n",
    "            try:\n",
    "                conn = http.client.HTTPSConnection(\"api.chatanywhere.com.cn\")\n",
    "                payload = json.dumps({\n",
    "                    \"max_tokens\": 512,\n",
    "                    \"model\": \"gpt-3.5-turbo\",\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "                headers = {\n",
    "                    'Authorization': 'Bearer sk-5szlvRHdRzZvQfum20165bFa0364427bA0B08cAf8765D52e',\n",
    "                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
    "                    'Content-Type': 'application/json'\n",
    "                }\n",
    "                conn.request(\"POST\", \"/v1/chat/completions\", payload, headers)\n",
    "                res = conn.getresponse()\n",
    "                data = res.read().decode(\"utf-8\")\n",
    "                data = json.loads(data)\n",
    "                response = data['choices'][0]['message']['content']\n",
    "                # trim function\n",
    "                if self._trim:\n",
    "                    response = _trim_preface_of_body(response)\n",
    "                return response\n",
    "            except Exception:\n",
    "                time.sleep(2)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27817cdec2cedfc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Implement a 'SandBox' interface\n",
    "**Caution**:\n",
    "The Sandbox interface implementation references https://github.com/RayZhhh/funsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d88a87535b6b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from implementation import evaluator\n",
    "from implementation import evaluator_accelerate\n",
    "\n",
    "\n",
    "class Sandbox(evaluator.Sandbox):\n",
    "    \"\"\"Sandbox for executing generated code. Implemented by RZ.\n",
    "\n",
    "    RZ: Sandbox returns the 'score' of the program and:\n",
    "    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).\n",
    "    2) stops the execution of the code in time (avoid endless loop).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=False, numba_accelerate=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            verbose         : Print evaluate information.\n",
    "            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions\n",
    "                              support numba acceleration, such as np.piecewise().\n",
    "        \"\"\"\n",
    "        self._verbose = verbose\n",
    "        self._numba_accelerate = numba_accelerate\n",
    "\n",
    "    def run(\n",
    "            self,\n",
    "            program: str,\n",
    "            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')\n",
    "            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.\n",
    "            inputs: Any,  # refers to the dataset\n",
    "            test_input: str,  # refers to the current instance\n",
    "            timeout_seconds: int,\n",
    "            **kwargs  # RZ: add this\n",
    "    ) -> tuple[Any, bool]:\n",
    "        \"\"\"Returns `function_to_run(test_input)` and whether execution succeeded.\n",
    "\n",
    "        RZ: If the generated code (generated by LLM) is executed successfully,\n",
    "        the output of this function is the score of a given program.\n",
    "        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.\n",
    "        \"\"\"\n",
    "        dataset = inputs[test_input]\n",
    "        try:\n",
    "            result_queue = multiprocessing.Queue()\n",
    "            process = multiprocessing.Process(\n",
    "                target=self._compile_and_run_function,\n",
    "                args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)\n",
    "            )\n",
    "            process.start()\n",
    "            process.join(timeout=timeout_seconds)\n",
    "            if process.is_alive():\n",
    "                # if the process is not finished in time, we consider the program illegal\n",
    "                process.terminate()\n",
    "                process.join()\n",
    "                results = None, False\n",
    "            else:\n",
    "                if not result_queue.empty():\n",
    "                    results = result_queue.get_nowait()\n",
    "                else:\n",
    "                    results = None, False\n",
    "\n",
    "            return results\n",
    "        except:\n",
    "            return None, False\n",
    "\n",
    "    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,\n",
    "                                  result_queue):\n",
    "        try:\n",
    "            # optimize the code (decorate function_to_run with @numba.jit())\n",
    "            if numba_accelerate:\n",
    "                program = evaluator_accelerate.add_numba_decorator(\n",
    "                    program=program,\n",
    "                    function_to_evolve=function_to_evolve\n",
    "                )\n",
    "            # compile the program, and maps the global func/var/class name to its address\n",
    "            all_globals_namespace = {}\n",
    "            # execute the program, map func/var/class to global namespace\n",
    "            exec(program, all_globals_namespace)\n",
    "            # get the pointer of 'function_to_run'\n",
    "            function_to_run = all_globals_namespace[function_to_run]\n",
    "            # return the execution results\n",
    "            results = function_to_run(dataset)\n",
    "            # the results must be int or float\n",
    "            if not isinstance(results, (int, float)):\n",
    "                result_queue.put((None, False))\n",
    "                return\n",
    "            result_queue.put((results, True))\n",
    "        except Exception:\n",
    "            # if raise any exception, we assume the execution failed\n",
    "            result_queue.put((None, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a05827354f9ae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. Prepare a 'specification'\r\n",
    "This code implements solution to the classic CVRP, which involves:\r\n",
    "1. Initializes all vehicles at the depot (node 0)\r\n",
    "2. For each vehicle:\r\n",
    "   - Selects next node using demand-aware priority scores\r\n",
    "   - Updates route until capacity is reached\r\n",
    "   - Returns to depot to finalize route\r\n",
    "4. Repeats until all customer demands served\r\n",
    "5. Evaluates total travel cost across all routes\r\n",
    "\r\n",
    "This code includes 3 components:\r\n",
    "1. `vehicle_routing`\r\n",
    "    - Constructs routes sequentially using a greedy insertion stratege\r\n",
    "    - Dynamically adjusts available nodes using\r\n",
    "        - Capacity checks: Verifies if next node's demand can be fulfilled\r\n",
    "\t- Exclusion logic: Prevents node revisits via masking\r\n",
    "    - Automatically dispatches new vehicles when capacity is exhausted\r\n",
    "2. `priority`\r\n",
    "    - Implements a weighted distance metric that considers:\r\n",
    "\t- Base travel cost (distance from current position)\r\n",
    "\t- Demand-adjusted scoring (demand used as penalty/reward)\r\n",
    "    - Favors nodes where:\r\n",
    "\t- Distance is short (minimizes travel time)\r\n",
    "\t- Demand fits remaining capacity (maximizes utilization)\r\n",
    "3. `evaluate`\r\n",
    "    - Computes average performance across mus negative cost convention for compatibility with maximmizations all routesroutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f875d128a693a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "specification = r'''\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "def vehicle_routing(max_capacity: int, node_demand: np.ndarray, \n",
    "                   cost_matrix: np.ndarray) -> tuple[list, float]:\n",
    "    \"\"\"\n",
    "    Solves the Capacitated Vehicle Routing Problem (CVRP) using heuristic approach.\n",
    "    \n",
    "    Args:\n",
    "        max_capacity: Maximum carrying capacity of each vehicle\n",
    "        node_demand: Array of demand values for each node (depot demand should be 0)\n",
    "        cost_matrix: Square matrix of inter-node travel costs\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (list of routes, total travel cost)\n",
    "        - routes: List where each sublist represents a vehicle's visitation sequence\n",
    "        - total_cost: Sum of all route distances including depot returns\n",
    "    \n",
    "    Note:\n",
    "        Depot is always assumed to be at index 0\n",
    "    \"\"\"\n",
    "    # Initialize routing variables\n",
    "    current_pos = 0  # Always start at depot (index 0)\n",
    "    remaining_cap = 0  # Current vehicle's remaining capacity\n",
    "    route_log = []  # Completed routes storage\n",
    "    temp_route = []  # Active route being built\n",
    "    total_cost = 0  # Cumulative distance across all routes\n",
    "    segment_cost = 0  # Distance for current route segment\n",
    "    processed_nodes = 0  # Counter for fulfilled demands\n",
    "    \n",
    "    # Prepare modified cost matrix\n",
    "    depot_returns = cost_matrix[:,0].copy()  # Save original depot return costs\n",
    "    working_matrix = cost_matrix.copy()\n",
    "    np.fill_diagonal(working_matrix, 1e10)  # Block self-loops\n",
    "    working_matrix[:,0] = 1e10  # Prevent premature depot returns\n",
    "\n",
    "    def _select_candidate(score_data: np.ndarray, excluded: list) -> int:\n",
    "        \"\"\"\n",
    "        Selects optimal next node from scored candidates while respecting exclusions.\n",
    "        \n",
    "        Args:\n",
    "            score_data: Priority scores for all nodes\n",
    "            excluded: List of nodes to exclude from selection\n",
    "            \n",
    "        Returns:\n",
    "            int: Index of selected node\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: When no valid nodes remain for selection\n",
    "        \"\"\"\n",
    "        # Flatten nested exclusion lists and remove duplicates\n",
    "        unique_exclusions = set(chain(*excluded))\n",
    "        valid_mask = np.ones_like(score_data, dtype=bool)\n",
    "        valid_mask[list(unique_exclusions)] = False\n",
    "        \n",
    "        # Early termination if no valid options\n",
    "        if np.sum(valid_mask) == 0:\n",
    "            raise RuntimeError(\"Exhausted all city options\")\n",
    "        \n",
    "        # Find highest score among remaining candidates\n",
    "        valid_scores = score_data[valid_mask]\n",
    "        peak_loc = np.argmax(valid_scores)\n",
    "        return np.flatnonzero(valid_mask)[peak_loc]\n",
    "\n",
    "    # Main routing loop - continues until all demands are processed\n",
    "    while processed_nodes < node_demand.size - 1:\n",
    "        # Combine completed routes and current partial route for exclusion\n",
    "        selection_restrictions = route_log + [[current_pos] + temp_route]\n",
    "        \n",
    "        # Get priority scores for candidate nodes\n",
    "        candidate_scores = priority(\n",
    "            current_pos, working_matrix.copy(),\n",
    "            max_capacity - remaining_cap, node_demand\n",
    "        )\n",
    "        \n",
    "        # Select next node using exclusion logic\n",
    "        chosen = _select_candidate(candidate_scores, selection_restrictions)\n",
    "        \n",
    "        # Validate selection against capacity and depot constraints\n",
    "        capacity_check = remaining_cap + node_demand[chosen] <= max_capacity\n",
    "        valid_destination = chosen != 0\n",
    "        \n",
    "        if not (capacity_check and valid_destination):\n",
    "            # Finalize current route and return to depot\n",
    "            segment_cost += depot_returns[current_pos]\n",
    "            total_cost += segment_cost\n",
    "            route_log.append(temp_route)\n",
    "            \n",
    "            # Reset for new vehicle route\n",
    "            current_pos = 0\n",
    "            segment_cost = 0\n",
    "            remaining_cap = 0\n",
    "            temp_route = []\n",
    "            continue\n",
    "        \n",
    "        # Update route with valid node\n",
    "        segment_cost += working_matrix[current_pos, chosen]\n",
    "        remaining_cap += node_demand[chosen]\n",
    "        temp_route.append(chosen)\n",
    "        processed_nodes += 1\n",
    "        \n",
    "        # Prevent revisiting selected node\n",
    "        working_matrix[:, chosen] = 1e10\n",
    "        current_pos = chosen\n",
    "    \n",
    "    # Handle final route if vehicle hasn't returned to depot\n",
    "    if current_pos != 0:\n",
    "        segment_cost += depot_returns[current_pos]\n",
    "        total_cost += segment_cost\n",
    "        route_log.append(temp_route)\n",
    "    \n",
    "    return route_log, total_cost\n",
    "\n",
    "@funsearch.run\n",
    "def evaluate(test_instances: dict) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates routing performance across multiple problem instances.\n",
    "    \n",
    "    Args:\n",
    "        test_instances: Dictionary of CVRP instances where each value contains:\n",
    "            - 'capacity': Vehicle capacity\n",
    "            - 'demand': Node demand array\n",
    "            - 'edge_weight': Cost matrix\n",
    "            \n",
    "    Returns:\n",
    "        float: Negative mean cost across all instances (for maximization)\n",
    "    \n",
    "    Note:\n",
    "        Returns negative cost because funsearch maximizes objectives\n",
    "    \"\"\"\n",
    "    return -np.mean(\n",
    "        vehicle_routing(\n",
    "            instance['capacity'],\n",
    "            instance['demand'],\n",
    "            instance['edge_weight']\n",
    "        )[1] for instance in test_instances.values()\n",
    "    )\n",
    "\n",
    "@funsearch.evolve\n",
    "def priority(current_position: int, dist_matrix: np.ndarray, \n",
    "             available_space: int, city_needs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes node selection priorities based on spatial and demand factors.\n",
    "    \n",
    "    Args:\n",
    "        current_position: Index of current node location\n",
    "        dist_matrix: Current distance/cost matrix\n",
    "        available_space: Remaining vehicle capacity\n",
    "        city_needs: Demand values for all nodes\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Priority scores for all nodes (higher = better)\n",
    "    \n",
    "    Logic:\n",
    "        - Reduces priority for nearby high-demand nodes when capacity allows\n",
    "        - Increases priority penalty when capacity insufficient\n",
    "        - Returns negative values for minimization conversion\n",
    "    \"\"\"\n",
    "    base_scores = dist_matrix[current_position].copy()\n",
    "    \n",
    "    for idx in range(base_scores.size):\n",
    "        demand = city_needs[idx]\n",
    "        if available_space >= demand:\n",
    "            # Favor nodes where demand can be satisfied\n",
    "            base_scores[idx] -= math.sqrt(demand)  # Distance-weight tradeoff\n",
    "        else:\n",
    "            # Penalize nodes that exceed capacity\n",
    "            adjustment = math.sqrt(demand)\n",
    "            base_scores[idx] += adjustment if base_scores[idx] > 0 else -adjustment\n",
    "    \n",
    "    return -base_scores  # Convert to negative for minimization\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bfe61e1661e18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. Prepare a dataset\n",
    "Our project use [**CVRPLib**](http://vrp.atd-lab.inf.puc-rio.br/index.php/en/ ) setB for testing.\n",
    "\n",
    "CVRPLib is the go-to benchmark dataset for CVRP, offering standardized instances with known optimal solutions that enable reliable algorithm comparisons. While invaluable for research and education, it focuses mainly on small-to-medium scale, static problems with homogeneous fleets, lacking real-world complexities like dynamic conditions or large-scale scenarios. Though limited for modern logistics applications, it remains essential for validating fundamental routing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea85ccfc8c0ca6d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'cvrplib/setE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m cvrp_dataset \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m cvrp_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dataset_path):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.vrp\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     10\u001b[0m         instances \u001b[38;5;241m=\u001b[39m vrplib\u001b[38;5;241m.\u001b[39mread_instance(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, file))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'cvrplib/setE'"
     ]
    }
   ],
   "source": [
    "import vrplib\n",
    "import os\n",
    "\n",
    "dataset_path = \"cvrplib/setE\"\n",
    "cvrp_dataset = {}\n",
    "cvrp_dataset['B'] = {}\n",
    "\n",
    "for file in os.listdir(dataset_path):\n",
    "    if file.endswith(\".vrp\"):\n",
    "        instances = vrplib.read_instance(os.path.join(dataset_path, file))\n",
    "        cvrp_dataset['B'][file[:-4]] = instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66651fb2764ce9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 5. Start FunSearch\n",
    "Please note that in jupyter notebook the following code will fail. This is because juypter does not support multiprocessing. Colab backend supports multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ec0c796d09ca1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from implementation import funsearch\n",
    "from implementation import config\n",
    "\n",
    "# It should be noted that the if __name__ == '__main__' is required.\n",
    "# Because the inner code uses multiprocess evaluation.\n",
    "if __name__ == '__main__':\n",
    "    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)\n",
    "    config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=30)\n",
    "    global_max_sample_num = 100  # if it is set to None, funsearch will execute an endless loop\n",
    "    funsearch.main(\n",
    "        specification=specification,\n",
    "        inputs=bin_packing_or3,\n",
    "        config=config,\n",
    "        max_sample_nums=global_max_sample_num,\n",
    "        class_config=class_config,\n",
    "        log_dir='../logs/funsearch_llm_api'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
